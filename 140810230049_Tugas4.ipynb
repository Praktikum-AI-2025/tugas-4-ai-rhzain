{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dy5aqLlqK1Yk"
      },
      "outputs": [],
      "source": [
        "# =====================================================================================================\n",
        "# Membangun sebuah model Neural Network untuk klasifikasi dataset Horse or Human dalam binary classes.\n",
        "#\n",
        "# Input layer harus menerima 150x150 dengan 3 bytes warna sebagai input shapenya.\n",
        "# Jangan menggunakan lambda layers dalam model.\n",
        "#\n",
        "# Dataset yang digunakan dibuat oleh Laurence Moroney (laurencemoroney.com).\n",
        "#\n",
        "# Standar yang harus dicapai untuk accuracy dan validation_accuracy > 83%\n",
        "# =====================================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def solution_05():\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
        "    local_file = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/horse-or-human')\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
        "    local_file = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/validation-horse-or-human')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = 'data/horse-or-human'\n",
        "    VALIDATION_DIR = 'data/validation-horse-or-human'\n",
        "\n",
        "# Debug - Print directory contents to verify\n",
        "    train_horses = len(os.listdir(os.path.join(TRAINING_DIR, 'horses')))\n",
        "    train_humans = len(os.listdir(os.path.join(TRAINING_DIR, 'humans')))\n",
        "    val_horses = len(os.listdir(os.path.join(VALIDATION_DIR, 'horses')))\n",
        "    val_humans = len(os.listdir(os.path.join(VALIDATION_DIR, 'humans')))\n",
        "\n",
        "    print(f\"Training: {train_horses} horses, {train_humans} humans\")\n",
        "    print(f\"Validation: {val_horses} horses, {val_humans} humans\")\n",
        "\n",
        "    # Basic data augmentation - keep it simple\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1/255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Only rescaling for validation\n",
        "    validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "    # Create generators with moderate batch size\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        TRAINING_DIR,\n",
        "        target_size=(150, 150),  # Back to original size\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        VALIDATION_DIR,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    print(f\"Train generator samples: {train_generator.samples}\")\n",
        "    print(f\"Validation generator samples: {validation_generator.samples}\")\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # First block\n",
        "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        # Second block\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        # Third block\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        # Fourth block\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "        # Flatten and dense with strong regularization\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(512, activation='relu',\n",
        "                             kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid') # Don't change this line!\n",
        "    ])\n",
        "\n",
        "    # Use RMSprop with a moderate learning rate\n",
        "    model.compile(\n",
        "        optimizer=RMSprop(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Callbacks with proper monitoring\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'best_model.keras',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            mode='max',\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "    validation_steps = validation_generator.samples // validation_generator.batch_size\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=30,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_steps,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "VpEg8wARK8oK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_05()\n",
        "    model.save(\"model_05.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKp1ni2nc95n",
        "outputId": "fd825280-cb1e-4905-d683-8a887670d0a1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: 500 horses, 527 humans\n",
            "Validation: 128 horses, 128 humans\n",
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Train generator samples: 1027\n",
            "Validation generator samples: 256\n",
            "Steps per epoch: 51\n",
            "Validation steps: 12\n",
            "Epoch 1/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 274ms/step - accuracy: 0.5709 - loss: 1.4749 - val_accuracy: 0.8000 - val_loss: 1.0921 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8500 - loss: 0.9423 - val_accuracy: 0.7917 - val_loss: 1.1781 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 236ms/step - accuracy: 0.8214 - loss: 0.8919 - val_accuracy: 0.8083 - val_loss: 1.3339 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9000 - loss: 0.6124 - val_accuracy: 0.8417 - val_loss: 1.2472 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 229ms/step - accuracy: 0.8774 - loss: 0.6157 - val_accuracy: 0.7167 - val_loss: 2.7296 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9500 - loss: 0.3200 - val_accuracy: 0.6875 - val_loss: 3.2110 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 408ms/step - accuracy: 0.8962 - loss: 0.4577 - val_accuracy: 0.8458 - val_loss: 0.8915 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.2431 - val_accuracy: 0.8292 - val_loss: 1.1568 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 405ms/step - accuracy: 0.9328 - loss: 0.3318 - val_accuracy: 0.7500 - val_loss: 1.9971 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m 1/51\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.1711\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.1711 - val_accuracy: 0.7375 - val_loss: 2.2209 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 418ms/step - accuracy: 0.9556 - loss: 0.2437 - val_accuracy: 0.7417 - val_loss: 2.5549 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9500 - loss: 0.2672 - val_accuracy: 0.7667 - val_loss: 2.3985 - learning_rate: 5.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.9678 - loss: 0.2131\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 445ms/step - accuracy: 0.9678 - loss: 0.2129 - val_accuracy: 0.6667 - val_loss: 5.1082 - learning_rate: 5.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1175 - val_accuracy: 0.6708 - val_loss: 4.9894 - learning_rate: 2.5000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 472ms/step - accuracy: 0.9771 - loss: 0.1866 - val_accuracy: 0.7833 - val_loss: 2.4840 - learning_rate: 2.5000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m 1/51\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.1132\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.1132 - val_accuracy: 0.7833 - val_loss: 2.4927 - learning_rate: 2.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 233ms/step - accuracy: 0.9837 - loss: 0.1459 - val_accuracy: 0.7542 - val_loss: 3.1603 - learning_rate: 1.2500e-04\n",
            "\n",
            "--- Final Evaluation ---\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7834 - loss: 1.2449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final validation accuracy: 0.8458\n"
          ]
        }
      ]
    }
  ]
}